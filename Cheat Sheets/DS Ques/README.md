### Interview questions in statistics related to data science
* https://towardsdatascience.com/40-statistics-interview-problems-and-answers-for-data-scientists-6971a02b7eee 
* https://analyticsindiamag.com/40-interview-questions-on-statistics-for-data-scientists/

### ML fundamentals
* Bias variance trade-off
  * What is Bias and Variance
  * Few techniques to control bias and variance
  * What will be the effect of bagging on Bias and variance?
* Linear and Logistic Regression
  * What are the assumptions of linear regression?
  * What are the assumptions of linear regression?
  * What is Heteroskedasticity?
  * If we add square of a variable as a feature, is the model still linear?
* Tree Based Models
  * How does a decision tree work - What are the different splitting criteria used?
    * Handling missing values
    * Follow up questions on random forest
    * Bagging vs boosting
  * What does “random” stands for in random forest model
* PCA
  * Working Principle?
  * How to identify which eigen vector is the first, second … component etc.
* Parametric vs non-parametric with examples
  * Effective number of params in KNN.
  * Parameter vs Hyperparameters
* KNN vs K-means
  * What is Curse of Dimensionality?
  * Difference between parameters and Hyperparameters
  * How can we select an optimal ‘k’ in k-means algorithm?
  * What are some of the approaches to initialize ‘centroid’
* Evaluation metrics for classification
  * Precision vs Recall
  * AUC-ROC
    * Range of AUC-ROC (best and worst)
    * What will happen to AUCROC if we flip the labels before training the model?
    * What will happen to AUCROC if we flip the labels after training the model?
* Class Imbalance
  * What are some of the ways to handle class imbalance?
  * What metrics should we use in case of imbalance
* Outliers:
  * Effect of outlier in model performance
  * How to detect and treat outliers 
* How to check model overfitting and Ways to handle overfitting
* Expectation Maximization Algorithm
* Gaussian Mixture Model 
* What is Maximum Likelihood Estimate
* Difference between Likelihood and Probability
* Bayes’ Theorem. 
  * Meaning of Posterior, Prior and Likelihood
  * Naïve Bayes algorithm – What does Naïve keyword stands for?
* Matrix Decomposition/Factorization
  * SVD vs Eigen Decomposition
    * What is Truncated SVD?
    * In which scenario Eigen Decomposition is same as SVD
* What is difference between Bagging, Boosting and stacking
* 
Deep learning
* What is an objective or loss function? 
  * Domain of loss fn – model params 
  * How do we optimize this loss function?
    * Gradient descent 
    * Problem of local minima and how to avoid one
    * Different flavours of gradient descent – advantages of minibatch
  * Impact of learning rate – What will happen if we use a very high/very low learning rate?
* Why do we need activation function
* Difference between skip-gram and word2vec
* What is Backpropagation?
* Why are activation functions used? What is the advantage 
* Dropout layer – training and inference
* Weight Initialization
  * Case with constant value
* What are optimisers- Name and explain few 
* What is batch normalization?
* How do you handle Nan loss?





















