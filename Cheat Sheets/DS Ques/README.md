### Interview questions in statistics related to data science
* https://towardsdatascience.com/40-statistics-interview-problems-and-answers-for-data-scientists-6971a02b7eee 
* https://analyticsindiamag.com/40-interview-questions-on-statistics-for-data-scientists/

ML fundamentals
•	Bias variance trade-off
o	What is Bias and Variance
o	Few techniques to control bias and variance
o	What will be the effect of bagging on Bias and variance?
•	Linear and Logistic Regression
o	What are the assumptions of linear regression?
o	What are the assumptions of linear regression?
o	What is Heteroskedasticity?
o	If we add square of a variable as a feature, is the model still linear?
•	Tree Based Models
o	How does a decision tree work - What are the different splitting criteria used?
	Handling missing values
	Follow up questions on random forest
	Bagging vs boosting
o	What does “random” stands for in random forest model
•	PCA
o	Working Principle?
o	How to identify which eigen vector is the first, second … component etc.
•	Parametric vs non-parametric with examples
o	Effective number of params in KNN.
o	Parameter vs Hyperparameters
•	KNN vs K-means
o	What is Curse of Dimensionality?
o	Difference between parameters and Hyperparameters
o	How can we select an optimal ‘k’ in k-means algorithm?
o	What are some of the approaches to initialize ‘centroid’
•	Evaluation metrics for classification
o	Precision vs Recall
o	AUC-ROC
	Range of AUC-ROC (best and worst)
	What will happen to AUCROC if we flip the labels before training the model?
	What will happen to AUCROC if we flip the labels after training the model?
•	Class Imbalance
o	What are some of the ways to handle class imbalance?
o	What metrics should we use in case of imbalance
•	Outliers:
o	Effect of outlier in model performance
o	How to detect and treat outliers 
•	How to check model overfitting and Ways to handle overfitting
•	Expectation Maximization Algorithm
•	Gaussian Mixture Model 
•	What is Maximum Likelihood Estimate
•	Difference between Likelihood and Probability
•	Bayes’ Theorem. 
o	Meaning of Posterior, Prior and Likelihood
o	Naïve Bayes algorithm – What does Naïve keyword stands for?
•	Matrix Decomposition/Factorization
o	SVD vs Eigen Decomposition
	What is Truncated SVD?
	In which scenario Eigen Decomposition is same as SVD
•	What is difference between Bagging, Boosting and stacking
•	
Deep learning
•	What is an objective or loss function? 
o	Domain of loss fn – model params 
o	How do we optimize this loss function?
	Gradient descent 
	Problem of local minima and how to avoid one
	Different flavours of gradient descent – advantages of minibatch
o	Impact of learning rate – What will happen if we use a very high/very low learning rate?
•	Why do we need activation function
•	Difference between skip-gram and word2vec
•	What is Backpropagation?
•	Why are activation functions used? What is the advantage 
•	Dropout layer – training and inference
•	Weight Initialization
o	Case with constant value
•	What are optimisers- Name and explain few 
•	What is batch normalization?
•	How do you handle Nan loss?

![image](https://user-images.githubusercontent.com/7243652/128911427-4ec6f134-fdf9-43bd-81c2-ea8c8862435d.png)





















