Regression is a method of modelling a target value based on independent predictors. Explains the degree of relationship between 2 or more variables using the best fit line.  <br/>
Regression indicates the impact of a unit change in know/independent variable on the dependent variable.  <br/>
Residual: Difference between Observed & Predicted <br/>
Error: Difference between Observed & True Value (Unobserved Population mean - very often unobserved, generated by the DGP) <br/>

### Assumptions
https://faculty.fuqua.duke.edu/~rnau/Decision411_2007/testing.htm
##### 1. Linearity
There should be a linear and additive relationship between dependent (response) variable and independent (predictor) variable(s). A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹. An additive relationship suggests that the effect of X¹ on Y is independent of other variables. <br/>
* Residual(Diff in actual & pred) vs Fitted values(Pred): Linearity assumption is satisfied if residuals are spread out randomly around the 0 line <br/>
To overcome the issue of non-linearity, you can do a non linear transformation of predictors such as log (X), √X or X² transform the dependent variable.
##### 2. No Autocorrelation
There should be no correlation between the residual terms. Absence of this phenomenon is known as Autocorrelation. <br/>
If the error terms are correlated, the estimated standard errors tend to underestimate the true standard error. If this happens, it causes confidence intervals and prediction intervals to be narrower.  <br/>
This usually occurs in time series models where the next instant is dependent in the previous instant. <br/>
* Durbin – Watson (DW) statistic: It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation. <br/>
* Residual vs time plot and look for the seasonal or correlated pattern in residual values. <br/>
##### 3. No Multicollinearity
The independent variables should not be correlated. Absence of this phenomenon is known as multicollinearity. <br/>
Standard errors tend to increase in presence of multicollinearity. With large standard errors, the confidence interval becomes wider leading to less precise estimates of slope parameters. <br/>
The existence of collinearity inflates the variances of the parameter estimates, and consequently incorrect inferences about relationships between explanatory and response variables.
* VIF factor for each variable is provided: VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity.  <br/>
VIF = 1/(1-R²). R² ranges from 0 to 1 so higher R² implies higher VIF implies variables are correlated
* VIF: Each of the predcitor variables are regressed up on other predictor variables. If that R² is high then this variables has collinearity with others. 
##### 4. Homoskedasticity
The error terms must have constant variance. This phenomenon is known as homoskedasticity. The presence of non-constant variance is referred to heteroskedasticity. <br/>
Heteroskedasticity i.e non-constant variance often creates cone-line shape scatter plot of residuals vd fitted. Scattering widens or narrows as the value of fitted increases implies it is consistently accurate when it predicts low values, but highly inconsistent in accuracy when it predicts high values. <br/>
* Residual(Diff in actual & pred) vs Fitted values(Pred) Plot: Should not be cone-shaped/funnel shaped <br/>
* Scale Location Plot ( Square root of Standardized residuals vs Theoritical Quantiles) <br/>
To overcome heteroskedasticity, a possible way is to transform the response variable such as log(Y) or √Y. Also, you can use weighted least square method to tackle heteroskedasticity. <br/>
##### 5. Normal Distribution of error terms
The error terms must be normally distributed. If not, confidence intervals may become too wide or narrow. Once confidence interval becomes unstable, it leads to difficulty in estimating coefficients based on minimization of least squares. <br/>
* Q-Q plot (Standardized residuals vs Theoritical Quantiles) <br/>
If the errors are not normally distributed, non–linear transformation of the variables (response or predictors) can bring improvement in the model.
##### 6. No outliers
Linear Regression is very sensitive to Outliers. It can terribly affect the regression line and eventually the forecasted values.

### OLS, MLE, Gradient descent are ways of getting parameters for a mode

### Ordinary Least Sqaure (OLS) Algorithm
https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76
* OLS is used to find estimators that minimizes residual sum of squares <br/>
* It is used in python library sklearn.  <br/>
* m (Slope for single independant var) = Σ(Xi-Xavg)(Yi-Yavg) / Σ(Xi-Xavg)²
* constant in y =mx+c is Yavg - (m * Xavg)

### Maximum Likelihood Estimation (MLE) is used to fit  Logistic regression as it involves probabilities
* MLE is used to find estimators that minimizes the likelihood function
* Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed. The parameter values that we find are called the maximum likelihood estimates (MLE).
* When is least squares minimisation the same as maximum likelihood estimation?
Least squares minimisation is another common method for estimating parameter values for a model in machine learning. It turns out that when the model is assumed to be Gaussian as in the examples above, the MLE estimates are equivalent to the least squares method. 

### Gradient Descent Algorithm
https://www.analyticsvidhya.com/blog/2018/10/introduction-neural-networks-deep-learning/ <br/>
https://www.coursera.org/learn/launching-machine-learning/lecture/EuGsr/gradient-descent <br/>
Search for global minimum. This can be done by finding the gradient of the loss function, and multiplying that with a hyper parameter, learning rate (helps determine the step size), and then subtracting that value from the current weights. This process iterates until convergence. <br/>
Main objective is to minimize cost function. <br/>
A gradient measures how much the output of a function changes if you change the inputs a little bit. <br/>
Gradient descent is an optimization algorithm that finds the optimal weights (a,b) (Equation: Y=a+bX) that reduces prediction error i.e difference between actual and predicted values. Steps:
1. Initialize the weights(a & b) with random values and calculate Error (SSE)
2. Calculate the gradient i.e. change in SSE when the weights (a & b) are changed by a very small value from their original randomly initialized value. This helps us move the values of a & b in the direction in which SSE is minimized. <br/>
   * Slope from the graph of (Loss vs Weight) suggests the direction and stepsize. If slope is +ve, decrease weights, if Magnitude of slope is more, step size should be a greater value <br/>
   * stepsize = slope * learning rate
   * The “learning rate” mentioned above is a flexible parameter which heavily influences the convergence of the algorithm. Larger learning rates make the algorithm take huge steps down the slope and it might jump across the minimum point thereby missing it. So, it is always good to stick to low learning rate such as 0.01. It can also be mathematically shown that gradient descent algorithm takes larger steps down the slope if the starting point is high above and takes baby steps as it reaches closer to the destination to be careful not to miss it and also be quick enough. <br/>
   * Learning Rate: Determines how fast or slow we will move towards the optimal weights. In order for Gradient Descent to reach the local minimum, we have to set the learning rate to an appropriate value, which is neither too low nor too high. This is because if the steps it takes are too big, it maybe will not reach the local minimum and if you set the learning rate to a very small value, gradient descent will eventually reach the local minimum but it will maybe take too much time. <br/>
3. Adjust the weights with the gradients to reach the optimal values where SSE is minimized
4. Use the new weights for prediction and to calculate the new SSE
5. Repeat steps 2 and 3 till further adjustments to weights doesn’t significantly reduce the Error <br/>

##### Batch Gradient Descent
It is also called vanilla gradient descent, calculates the error for each example within the training dataset, but only after all training examples have been evaluated, the model gets updated. This whole process is like a cycle and called a training epoch. <br/>
Parameters are updated only after evaluating all examples within training set are evaluated. Takes big, slow steps. <br/> 
It produces a stable error gradient and a stable convergence. <br/>
It is preferable for small datasets. <br/>
##### Stochastic Gradient Descent
Parameters are updated for each example within the training set, one by one. It takes small, quick steps. <br/>
The frequent updates (computationally expensive) allow us to have a pretty detailed rate of improvement. <br/>
It is preferable for large datasets. <br/>
SGD randomly picks one data point from the whole data set at each iteration to reduce the computations enormously.
##### Mini Batch Gradient Descent
Training time is proportional to the number of data points, frequency with we check the loss <br/>
So, try decreasing the number of data points using mini batch gradient descent <br/>
It simply splits the training dataset into small batches (sampling) and performs an update on the parameters for each of these batches. <br/> 
It is also common to sample a small number of data points instead of just one point at each step and that is called “mini-batch” gradient descent.
Common mini-batch sizes range between 50 and 256. <br/>

### Dimensionality Reduction
https://quantifyinghealth.com/stepwise-selection/
##### Forward Selection
Starts with most significant predictor in the model and adds variable for each step. Forward selection starts with no predictors and adds one by one, and stops the process when the next addition is not statistically significant.
##### Backward Elimination
Starts with all predictors in the model and removes the least significant variable for each step. Backward select starts with all the predictors and starts removing them one by one, and stops when all the remaining variables are statistically significant.
##### Stepwise Selection
It does two things. It adds and removes predictors as needed for each step. Step-wise selection process we add one at each step and also remove when some thing does not stay significant.

### Regularization
For a model, training losss should be decreased but not at the expense of Complexity, as it overfitting. Regularization refers to any technique that helps generalize a model. A generalized model performs well not just on training data but also on never seen test data.<br/>
* Early stopping - Stop at the point where training error dec but Test error inc
* Parameter Norm Penalities
  * L1
  * L2
  * Max-norm
* Dataset Augmentation
* Noise Robustness
* Sparse Representations
<br/>
Regularization basically adds the penalty as model complexity increases. Regularization is used to prevent the model from overfitting the training sample. It constrains/regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. In regularization, we normally keep the same number of features, but reduce the magnitude of the coefficients. Regularization should be applied after standardizing features. L1 & L2 picks up vars that has greater values. So standardization helps. <br/>
<br/>
L1 and L2 regularization methods represent model complexity as the magnitude of the weight vector, and try to keep that in check. <br/>

#####  Ridge Regression - L2 Regularization
Ridge Regression is a technique used when the data suffers from multicollinearity ( independent variables are highly correlated). It solves the multicollinearity problem through shrinkage parameter λ (lambda), shrinks the value of coefficients but doesn’t reaches zero. <br/>
Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. <br/>
Minimization objective = Least Squares Obj + λ * (sum of square of coefficients) <br/>
The reason for doing that is to “punish” the loss function for high values of the coefficients β <br/> 
Magnitude of coefficients decreases as λ increases. λ basically controls penality term in the cost function of ridge reg. <br/>
R² for a range of λ and choose the one that gives higher R². <br/>
If the penality is very large it means model is less complex, therefore the bias would be high. <br/>
* Assumptions of this regression is same as least squared regression except normality is not to be assumed. <br/>
#####  Lasso Regression - L1 Regularization
Coefficients are reducing to 0 even for smaller changes in λ. Lasso selects the only some feature while reduces the coefficients of others to zero. This property is known as feature selection and which is absent in case of ridge. <br/>
Least Absolute Shrinkage and Selection Operator (Lasso) adds “absolute value of magnitude” of coefficient as penalty term to the loss function. <br/>
Minimization objective = Least Squares Obj + α * (sum of absolute value of coefficients) <br/>
If group of predictors are highly correlated, lasso picks only one of them and shrinks the others to zero. <br/>
* Assumptions of this regression is same as least squared regression except normality is not to be assumed. <br/>
Zeroing out coeffs can help with performance, especially with large models and sparse inputs
* With fewer coefficients to store and load, there is a reduction in storage and memory needed with a much smaller model size, which is especially important for embedded models. 
* Also, with fewer features, there are a lot fewer mult ads which not only leads to increased training speed, but more importantly increase prediction speed.

##### Elastic Net Regression
It generally works well when we have a big dataset. <br/>
Elastic net is basically a combination of both L1 and L2 regularization. <br/>
Elastic regression working: Let’ say, we have a bunch of correlated independent variables in a dataset, then elastic net will simply form a group consisting of these correlated variables. Now if any one of the variable of this group is a strong predictor (meaning having a strong relationship with dependent variable), then we will include the entire group in the model building, because omitting other variables (like what we did in lasso) might result in losing some information in terms of interpretation ability, leading to a poor model performance. <br/>

### Evaluation Metrics 
Square of errors i.e actual - prediction is done to treat both +ve values & -ve values similarly. <br/>
RMSE is more meaningful as square root of squared values are in the same measurement units as Actual values <br/>

##### Sum of Squared Errors (SSE) = =  {Σ(Y – Ypred)²} <br/>
##### RMSE - Root Mean Squared Error (Loss function) - Lower RMSE indicates better model
It is the sample standard deviation of the differences between predicted values and observed values (called residuals).  <br/>
RMSE = ( {Σ(Y – Ypred)²} / n )^0.5 <br/>
RMSE is higher or equal to MAE and is the default metric in most models because loss function defined in terms of RMSE is smoothly differentiable and makes it easier to perform mathematical operations. <br/>
Minimizing the squared error over a set of numbers results in finding its mean. <br/>
##### MAE - Mean Absolute error
Sum of the absolute difference between predictions and actual values <br/> 
MAE = (Σ|Y – Ypred|) / n  <br/>
Minimizing the absolute error results in finding its median. <br/> 
##### MSE - Mean Squared error (Loss function)
MSE = {Σ(Y – Ypred)²} / n  <br/>
##### R Squared (R²) / Coeffiecient of Determination
It is the proportion of variance in the response variable that is explained by the independant variables. It represents how close the data values are to the fitted regression line. <br/>
It doesn't determine variable importance. <br/> 
Ranges from 0 to 1 and are commonly stated as percentages from 0% to 100%. <br/>
R² = 1 - (Explained Variation by model / Total Variation) # Value is between 0 & 1<br/>
Explained variation by model (It is not the complete formulae for variance) = Σ(Y – Ypred)² <br/> 
Total Variation (It is not the complete formulae for variance) = Σ(Y – Yavg)² <br/>
##### Adjusted R Squared (R²)
R² assumes that every single variable explains the variation in the dependent variable. R² either stay the same or increase with addition of more variables wven if they dont have any relationship with the output variables. <br/> 
The adjusted R² tells you the percentage of variation explained by only the independent variables that actually affect the dependent variable. It penalizes you for adding variables which dont improve your existing model. If the R2 increases by a significant value, then the adjusted r-squared would increase. If there is no significant change in R2, then the adjusted r2 would decrease.<br/> 
The best regression model is the one with the largest adjusted R2-value. <br/> 
R-squared = 1-(SSE/SST) = SSR/SST <br/> 
R-square and Adjusted R squared would be exactly same for single input variable. <br/> 
Adjusted R² = 1 - { (1-R²)(n-1)/(n-k-1) }    <br/>
n is total sample size and k is no of predictors <br/>


##### Mallow's Cp
Mallow’s Cp is a technique for model selection in regression. The Cp statistic is defined as a criteria to assess fits when models with different number of parameters are being compared. A small value of Cp means that the model is relatively precise. <br/>
Mallows’s Cp has been shown to be equivalent to Akaike information criterion in the special case of Gaussian linear regression. <br/>
Mallows’s Cp addresses the issue of overfitting, in which model selection statistics such as the residual sum of squares always get smaller as more variables are added to a model. <br/>
##### AIC - Akaike's Info Criterion
AIC is an estimator of the relative quality of statistical models for a given set of data. <br/>
AIC estimates the relative information lost by a given model: the less information a model loses, the higher the quality of that model. <br/>
AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model. <br/>
AIC and BIC are both penalized-likelihood criteria. Both are of the form “measure of fit + complexity penalty”. Lowest value implies best model <br/>
AIC = -2*ln(likelihood) + 2*p 
##### BIC - Bayesian Info Criterion
Both BIC and AIC attempt to resolve overfitting problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC. <br/>
BIC = -2*ln(likelihood) + ln(N)*p

### Bullet Points
* OLS Regression results interpretation: https://medium.com/@jyotiyadav99111/statistics-how-should-i-interpret-results-of-ols-3bde1ebeec01
* Linear regression will have high bias and low variance
* Least Square Error is used to identify the line of best fit (minimizes sum of squares prediction errors)
* Overfitting is more likeley when we 
* Sum of residuals is always 0 and so is mean in linear reg
* If a degree k polynomial perfectly fits the data, there are high chance that degree k+ polynomial will overfit the data and degree k- will underfit the data
* Higher degree polynomials over-fit the data
* Maximum Likelihood estimate (MLE) may not always exist. If MLE exist, it (they) may not be unique.
* F-test in Linear reg is to test the goodness of the model
* SSR (Sum of Squares Regression) = Σ(Ypred – Yavg)². Greater the value better the regression model
* SSE (Sum of Squares errors) = Σ(Y – Ypred)² # Residual error
* SST (Sum of Squares total) = Σ(Y – Yavg)² # Explains variation within the dependent variable





